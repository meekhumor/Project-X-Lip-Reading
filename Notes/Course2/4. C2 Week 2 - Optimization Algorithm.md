### Batch v/s mini-batch gradient descent:
- Batch Gradient Descent processes the entire training set before taking a single step in gradient descent, which can be slow for large datasets.
- Mini-batch gradient descent,is an optimization algorithm that can significantly speed up training for neural networks, especially when dealing with large datasets.
- In mini-batch gradient decent the training set is split into smaller "mini-batches"using vectorization to make the training process faster. Each mini-batch is processed independently, allowing for faster iterations.

![mini v batch](https://github.com/user-attachments/assets/2078295f-0c32-436b-bf21-379b715d4602)

- Processing Mini-Batches
  - For each mini-batch \( (X^{(t)}, Y^{(t)}) \):
   - 1. Implement forward propagation.
   - 2. Compute the cost function for the mini-batch.
   - 3. Implement backpropagation to compute gradients.
   - 4. Update weights \( W \) and biases \( b \)

![mini batcch grad](https://github.com/user-attachments/assets/304f1645-1cfc-49e2-943e-7e8fa4eb1cb1)

### Training with mini batch gradient descent:
- The size of the mini-batch is an important parameter to consider.
- Batch Gradient Descent: When mini-batch size equals the training set size M, it's just batch gradient descent. Processing the entire training set on each iteration is time-consuming for large datasets.
- Stochastic Gradient Descent: When mini-batch size equals 1, each example is processed individually. This leads to high noise and inefficiency due to lack of vectorization, making it hard to converge.

_strongIdeally the size of the mini-batcj should be between 1 and M not too big or small.strong_

```
Small training set -> Batch gradient descent
Typical training set -> 64, 128, 256, 512 and 1024
```
![mini batch size](https://github.com/user-attachments/assets/edac027f-e614-40f2-99a1-63c493e88f3c)

### Exponentially weighted averages:
- Exponentially weighted averages is an optimization method used to find trends and patterns in noisy datasets using the concept of moving averages.
- The algorithm gives more emphasis to recent data and less to older data.
- Formula:
  - `\[ V_t = \beta V_{t-1} + (1 - \beta) \theta_t \]`
  - \( \beta \): Between 0 and 1.
  - \( \theta_t \): Value at time \( t \).

  - **High \( \beta \) (e.g., 0.98)**: Smoother, slower response, less noise.
  - **Moderate \( \beta \) (e.g., 0.9)**: Balanced smoothness and responsiveness.
  - **Low \( \beta \) (e.g., 0.5)**: Faster response, noisier.

![exp weight avg](https://github.com/user-attachments/assets/fd096eb8-0fae-4cc0-b66c-a447c28b3ba1)

#### Intuition and implementation:
![ewa intu](https://github.com/user-attachments/assets/eae45124-001e-49ab-aa88-1fcae0dca807)

![implement ewa](https://github.com/user-attachments/assets/138ad782-1132-4b24-804a-a57ab1aa44d5)

#### Bias correction:
- Bias correction is a technique used to improve the accuracy of exponentially weighted averages, especially during the initial phase when the estimate may be skewed or less accurate.
- When you first initialize the exponentially weighted average with zero, the early estimates can be biased low. Bias correction helps adjust these early estimates to be more accurate.
- Bias Correction Formula: 
  - `\[ V_t^{\text{corrected}} = \frac{V_t}{1 - \beta^t} \]`
  - \( V_t \) is the exponentially weighted average at time \( t \).
  - \( \beta \) is the smoothing factor.

```
- During the initial phase of the moving average, bias correction significantly improves the accuracy of the estimates.
- As \( t \) becomes large, the term \( \beta^t \) approaches zero, so the bias correction has less impact.
```

![bias correction](https://github.com/user-attachments/assets/0836d7bd-1c2f-421e-94a1-06cda12b892b)

