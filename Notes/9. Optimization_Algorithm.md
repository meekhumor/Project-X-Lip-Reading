### Batch v/s mini-batch gradient descent

- To speed up the training process, we can use a technique called mini-batch gradient descent.
- Instead of processing the entire training set at once, we split it into smaller subsets called mini-batches.
- Each mini-batch contains a smaller number of examples, making it faster to process.

![batch vs mini](https://github.com/user-attachments/assets/7b09850e-c231-440b-9642-231ef6815269)

![mini batch grad](https://github.com/user-attachments/assets/102dea14-958d-4091-985e-7e0967814fac)

#### Training with mini batch gradient descent

- The size of the mini-batch is an important parameter to consider.
- If the mini-batch size is equal to the size of the training set, it becomes batch gradient descent
- On the other hand, if the mini-batch size is equal to 1, it becomes **stochastic gradient descent**.
  - Disadvantage is that we loose all thespeeding from vectorization

_In practice, the mini-batch size is usually somewhere in between, not too big or too small, to balance efficiency and accuracy._


```
Small training set -> Batch gradient descent
Typical training set -> 64, 128, 256, 512 and 1024
```

### Exponentially weighted averages

- It's a way to calculate the average of a set of numbers, but with more emphasis on recent values.
- The weights decrease exponentially as we go back in time. So, the most recent data has the highest weight, and the weight decreases as we move further back in time.
- By using these weighted averages, we can smooth out the data and get a better sense of the overall trend in data.

`V(t) = β V(t-1) + (1 - β) θ(t)`